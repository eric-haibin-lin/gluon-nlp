BERT:
  CLUSTER:
    NP: 256
    HOST: hosts_256
  
  TRAIN:
    DTYPE: float32
    LOG_INTERVAL: 2
    OPTIMIZER: bertadam
    MODEL: bert_12_768_12
    CKPT_DIR: /data/results/v2-900g-ckpt
    CKPT_INTERVAL: 50
    EVAL_INTERVAL: 25
  
  HVD:
    CYCLE_TIME: 85
    HIERARCHICAL: 0

  NCCL:
    MIN_NUM_RINGS: 1
  
  RUN:
    PHASE1: 1
    PHASE2: 1
  
  PHASE2:
    DATA: /data/datasets/BERT_training_3_months_snapshot_unencrypted/sampled/train/*.txt,/data/datasets/BERT_training_impressed_asins_06_11_smaller_unencrypted/train/*.txt,
    DATA_EVAL: /data/datasets/generated/v0-smaller_unigram/*.npz,
    OPTIONS: --raw --skip_state_loading --start_step 0 --verbose --num_buckets 1 --sentencepiece /data/vocab/asin-unigram-32000-150M.model
    NUM_STEPS: 300
    BS: 256
    ACC: 1
    MAX_SEQ_LENGTH: 128
    MAX_PREDICTIONS_PER_SEQ: 20
    LR: 0.0004
    WARMUP_RATIO: 0.16

  ENV:
    SHARE_SEED: 0

CONTAINER:
  NAME: bert
  SHARED_FS: /fsx/asin-bert
  REGISTRY: haibinlin/bert-docker:ads
